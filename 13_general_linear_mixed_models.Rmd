---
title       : смешаные линейные модели
subtitle    : Линейные модели, осень 2015
author: Марина Варфоломеева, Вадим Хайтов
presenters: [{
    name: 'Марина Варфоломеева',
    company: 'каф. ЗБП, СПбГУ',
    }]
output:
  ioslides_presentation:
    widescreen: true
    css: my_styles.css
    logo: Linmod_logo.png
---

## Вы узнаете

- Что такое смешаные модели и когда они применяются
- Что такое фиксированные и случайные факторы

### Вы сможете

- Рассказать чем фиксированные факторы отличаются от случайных
- Привести примеры факторов, которые могут быть фиксированными или случайными в зависимости от задачи исследования
- Рассказать, что оценивает коэффициент внутриклассовой корреляции и вычислить его для  случая с одним случайным фактором
- Подобрать смешаную линейную модель со случайным отрезком и случайным углом наклона в R при помощи методов максимального правдоподобия



```{r setup, include = FALSE, cache = FALSE}
#-- RUN THE FRAGMENT BETWEEN LINES BEFORE COMPILING MARKDOWN
# to configure markdown parsing
options(markdown.extensions = c("no_intra_emphasis", "tables", "fenced_code", "autolink", "strikethrough", "lax_spacing", "space_headers", "latex_math"))
#------
# output options
options(width = 70, scipen = 6, digits = 3)

# to render cyrillics in plots use cairo pdf
options(device = function(file, width = 7, height = 7, ...) {
  cairo_pdf(tempfile(), width = width, height = height, ...)
  })
library(knitr)
# chunk default options
opts_chunk$set(fig.align='center', tidy = TRUE, fig.width = 7, fig.height = 3, message=FALSE, warning=FALSE)
```

# Многоуровневые данные

## Пример: Как время реакции людей зависит от бессонницы?

Данные из Belenky et al., 2003.  
В нулевой день эксперимента всем испытуемым давали поспать нормальное время. Начиная со следующей ночи давали спать по 3 часа. 

- `Reaction` - среднее время реакции в серии тестов в день наблюдения, мс
- `Days` - число дней депривации сна
- `Subject` - номер субъекта

```{r}
library(lme4)
data(sleepstudy)
sl <- sleepstudy
head(sl, 3)
```


## Знакомство с данными


```{r}
str(sl)
# пропущенные значения
sum(!complete.cases(sl))
# число субъектов
length(unique(sl$Subject))
```

## Знакомство с данными (продолжение)

```{r}
# сбалансирован ли объем выборки?
table(sl$Subject)
with(sl, table(Subject, Days))
```

## Есть ли выбросы?

```{r, tidy=FALSE}
library(ggplot2)
theme_set(theme_bw() + theme(legend.key = element_blank()))
update_geom_defaults("point", list(shape = 19))
# Есть ли наблюдения-выбросы? строим dot-plot
ggplot(sl, aes(x = Reaction, y = 1:nrow(sl), colour = Subject)) + 
  geom_point() + guides(colour = guide_legend(ncol = 2))
```
>- Субъектов с необычным временем реакции нет  
>- Видно, что у разных субъектов время реакции различается. Есть быстрые, есть медленные. Межиндивидуальную изменчивость нельзя игнорировать.

## Что делать с разными субъектами?

Неправильные варианты:

- игнорировать структуру данных, не учитывать группирующий фактор
- учитывать группирующий фактор как обычно 

Правильный вариант

- подобрать смешаную модель (в которой есть фиксированные и случайные факторы)

## Неправильный вариант 1. Не учитываем группирующий фактор.

$Reaction_{i} = \beta_0 + \beta_1 Days_{i} + \epsilon_{i}$

$\epsilon_i \sim N(0, \sigma^2)$  
$i = 1, 2, ..., 180$ -- общее число наблюдений

В матричном виде $\mathbf{Reaction} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$

```{r}
wrong1 <- lm(Reaction ~ Days, data = sl)
```

<div class="columns-2">
Если мы не учитываем группирующий фактор, все будет "очень достоверно" из-за низких стандартных ошибок.  
Но в этом случае условие независимости будет нарушено, поэтому все не так как кажется.

```{r, echo=FALSE, fig.height=2.8, fig.width=4.5}
ggplot(sl, aes(x = Days, y = Reaction)) + 
  geom_point() + 
  geom_smooth(se = F, method = "lm", size = 1)
```
</div>

## Неправильный вариант 2. Группирующий фактор как фиксированный.

$Reaction_{ij} = \beta_0 + \beta_1 Days_{j} + \beta_{2}Subject_{i = 2} + ... + \beta_{2}Subject_{i = `r length(unique(sl$Subject))`} + \epsilon_{ij}$

$\epsilon_ij \sim N(0, \sigma^2)$ - остатки от регрессии  
$i = 1, 2, ..., 18$ - субъект  
$j = 1, 2, ..., 10$ - день

В матричном виде $\mathbf{Reaction} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$

```{r}
wrong2 <- lm(Reaction ~ Days + Subject, data = sl)
```

Если мы учитываем группирующий фактор как обычно (как __фиксированный фактор__), придется оценивать слишком много параметров (`r length(unique(sl$Subject))` для уровней группирующего фактора, 1 для `Days`, $\epsilon$ --- всего `r length(coef(wrong2)) + 1`).  
При этом у нас всего `r sum(complete.cases(sl))` наблюдений. Чтобы получить удовлетворительную мощность, нужно минимум 10--20 наблюдений на каждый параметр (Harrell, 2013) --- у нас `r sum(complete.cases(sl))/(length(coef(wrong2)) + 1)`.

## Что нам делать с этим множеством прямых?

```{r, tidy=FALSE}
wrong2_diag <- fortify(wrong2)
ggplot(wrong2_diag, aes(x = Days, colour = Subject)) +
  geom_line(aes(y = .fitted, group = Subject)) +
  geom_point(data = sl, aes(y = Reaction)) +
  guides(colour = guide_legend(ncol = 2))
```

>- Нас не интересует, как различается время реакции каждого конкретного субъекта. Можем попытаться вместо подбора отдельных интерсептов, оценить разброс их значений.

## Можно посмотреть на группирующий фактор иначе!

Нам не важны конкретные значения на разных уровнях фактора. Мы можем представить, что эффект фактора --- случайная величина. Мы можем оценить дисперсию между уровнями группирующего фактора.

Такие факторы называются __случайными факторами__, а модели с такими факторами называются __смешаными моделями__:

- Общие смешаные модели (general linear mixed models) --- нормальное распределение зависимой переменной

- Обобщенные смешаные модели (generalized linear mixed models) --- другие формы распределений зависимой переменной

## Фиксированные и случайные факторы

Свойства | Фиксированные факторы | Случайные факторы
---------|---------|---------|
Уровни фактора | фиксированные, заранее определенные и потенциально воспроизводимые уровни | случайная выборка из всех возможных уровней |
Используются для тестирования гипотез | о средних значениях отклика между уровнями фактора<br />$H _{0}: μ _1 = μ _2 = · · · = μ _i = μ$ | о дисперсии отклика между уровнями фактора<br />$H _{0}: \sigma_{\alpha}^2 = 0$ |
Выводы можно экстраполировать | только на уровни из анализа | на все возможные уровни
Число уровней фактора | __Осторожно!__ Если уровней фактора слишком много, то нужно подбирать слишком много коэффициентов - должно быть много данных | __Важно!__ Для точной оценки $\sigma$ нужно нужно много уровней фактора - не менее 5 |

## Примеры фиксированных и случайных факторов

__Фиксированные факторы__

- Пол
- Низина/вершина
- Илистый/песчаный грунт
- Тень/свет
- Опыт/контроль

__Случайные факторы__

- Субъект, особь или площадка (если есть несколько измерений)
- Выводок
- Блок, делянка на участке
- Аквариум в лаб. эксперименте

## Какого типа эти факторы? От чего это зависит?

- Несколько произвольно выбранных градаций плотности моллюсков в полевом эксперименте, где плотностью манипулировали.

- Фактор размер червяка (маленький, средний, большой) в выборке червей.

- Деление губы Чупа на зоны с разной степенью распреснения.

# смешаные линейные модели

## смешаная линейная модель в общем виде

$$Y_i = X _i \times \boldsymbol{\beta} + Z_i \times b _i + \boldsymbol{\epsilon} _i$$

$\mathbf{b} _i \sim N(0, \mathbf{D})$ - случайные эффекты нормально распределены со средним 0 и дисперсией $d^2$

$\boldsymbol{\epsilon} _i \sim N(0, \boldsymbol{\sigma})$ - остатки модели нормально распределены со средним 0 и матрицей ковариаций $\boldsymbol{\sigma}$

$\mathbf{X} _i \times \boldsymbol{\beta}$ - фиксированная часть модели

$\mathbf{Z}_i \times \mathbf{b} _i$ - случайная часть модели

## В примере модель со случайным отрезком можно записать так: {.smaller}

$Reaction_{ij} = \beta_0 + \beta_1 Days_{ij} + b_i + \epsilon_{ij}$

$b_{i} \sim N(0, d^2)$ - случайный эффект субъекта (intercept)  
$\epsilon_{ij} \sim N(0, \boldsymbol{\sigma})$ - остатки модели  
$i = 1, 2, ..., 18$ - субъекты  
$j = 1, 2, ..., 10$ - дни

Для каждого субъекта $i$ в матричном виде это записывается:

$$ \left( \begin{array}{c} Reaction _{i1} \\ Reaction _{i2} \\ \vdots \\ Reaction _{i10} \end{array} \right)
= \left(\begin{array}{cc}
1 & Days _{i1} \\ 1 & Days _{i2} \\ \vdots \\ 1 & Days _{i10}
\end{array} \right)
\times
\left( \begin{array}{c}
\beta _0 \\ \beta _1
\end{array} \right) + 
\left( \begin{array}{c} 1 \\ 1 \\ \vdots \\ 1 \end{array} \right)
\times b _{i} + 
\left( \begin{array}{c} \epsilon _{i1} \\ \epsilon _{i2}\\ \vdots \\ \epsilon _{i10} \end{array} \right)$$

что можно записать сокращенно так:

$$\mathbf{Reaction} _i = \mathbf{X} _i \times \boldsymbol{\beta} + \mathbf{Z} _i \times \mathbf{b} _i + \boldsymbol{\epsilon}_i$$


## Подбор смешаных моделей в R {.smaller}

Самые популярные пакеты - `nlme` (старый, иногда медленный, стабильный, хорошо документированный) и `lme4` (новый, быстрый, не такой стабильный, хуже документированный). Есть много других.

Функция     | `lme()`<br />из `nlme` | `lmer()`<br />из `lme4` | `glmer()`<br />из `lme4` | `glmmPQL()`<br />из `MASS` |
------------|------------|------------|------------|------------|
Распределение отклика | нормальное  | нормальное | биномиальное, пуассоновское,<br />гамма, квази-...<br /><br /> | биномиальное, пуассоновское,<br />гамма, квази-...,<br />__отр. биномиальное__
Метод оценивания | ML, REML | ML, REML | ML, REML | PQL |
Гетерогенность дисперсий | + | - | - | - |
Корреляционные структуры | + | - | - | + |
Доверительная вероятность<br />(p-value) | + | - | - | + |

## Синтаксис для смешаных моделей в R

__Фиксированная часть модели__ задается обычной двухсторонней формулой

`Y ~ 1 + X1 + ... + Xn`

__Случайная часть модели__ - односторонняя формула. До вертикальной черты --- перечислены факторы, влияющие на случайный угол наклона. После вертикальной черты --- факторы, влияющие на случайный intercept. 

`~ 1 + X1 + ... + Xn |A`

Вложенные друг в друга факторы указываются от крупного к мелкому через "/" 

`~ 1 + X1 + ... + Xn |A/B/C`

Детали синтаксиса разных функций отличаются (см. следующий слайд с примерами формул)

## {.flexbox .vcenter .smaller}

Факторы     | `lme()` из `nlme` | `lmer()` из `lme4` |
------------|------------|------------|
А -- случ. intercept | `lme(fixed=Y~1,random=~1|A,data=dt)` |  `lmer(Y~1+(1|A),data=dt)` |
A -- случ. intercept, X -- фикс. | `lme(fixed=Y~X,random=~1|A,data=dt)` |  `lmer(Y~X+(1|A),data=dt)` |
A -- случ. intercept и угол накл. X | `lme(fixed=Y~X,random=~1+X|A, data=dt)` |  `lmer(Y~X+(1+X|A), data=dt)` |
A -- случ. intercept, A вложен в фикс.Х | `nlme(fixed=Y~X,random=~1|X/A, data=dt)` |`lmer(Y~X+(1|A:X), data=dt)`|
A и В -- случ. intercept, A и B независимы (crossed effects), X -- фикс. |  |  `lmer(Y~X+(1|A)+(1|B), data=dt)` |
A и В -- случ. intercept, B вложен в А (nested effects), уровни B повт. в группах по A, X -- фикс.| `lme(fixed=Y~X, random=~1|A/B, data=dt)` |  `lmer(Y~X+(1|A/B), data=dt)`<br />`lmer(Y~X+(1|A)+(1|A:B), data=dt)` |
A и В -- случ. intercept, B вложен в А (nested random effects), все уровни B уникальны, X -- фикс. | `lme(fixed=Y~X,random=~1|A/B, data=dt)` |  `lmer(Y~X+(1|A)+(1|B), data=dt)` |


## Подберем модель со случайным отрезком с помощью `lme()` из пакета `nlme`

```{r}
detach("package:lme4") # выгружаем lme4, чтобы не было конфликтов
library(nlme)
M1 <- lme(Reaction ~ Days, random = ~ 1 | Subject, data = sl)
```

## 1. Анализ остатков.  Все ли в порядке с моделью?

Нужно построить серию диагностических графиков.

1) График остатков от предсказанных значений
2) График остатков от ковариат в модели
3) График остатков от ковариат не вошедших в модель (нет ли других нужных переменных?), если есть
4) График остатков от времени, если есть
5) График остатков от координат проб, если есть

## 1) График остатков от предсказанных значений

```{r, tidy=FALSE}
# plot(M1)
sl$.stdresid <- resid(M1, type = "pearson")
sl$.fitted <- fitted(M1)
ggplot(sl) + geom_point(aes(x = .fitted, y = .stdresid, colour = Subject))
```

> - Есть большие остатки, гетерогенность дисперсий

## 2) График остатков от ковариат в модели {.smaller}

```{r, fig.width=10, tidy=FALSE}
p <- ggplot(data = sl, aes(y = .stdresid))
library(gridExtra)
grid.arrange(
  p + geom_point(aes(x = Days, colour = Subject)),
  p + geom_boxplot(aes(x = Subject)),
  ncol = 2)
```
> - Большой остаток у наблюдения 332 субъекта
> - Нелинейный паттерн
> - Гетерогенность дисперсий
> - Пока оставим все как есть

## 2. Проверяем, какие из фиксированных факторов влияют одним из трех вариантов:

(а) По значениям t-(или -z) статистики (по REML оценке)  
(б) F-критерий - приблизительный результат (REML оценка)  
(в) likelihood ratio test или AIC (ML оценка)  
- Либо попарное сравнение вложенных моделей при помощи likelihood ratio test
- Либо сравнение моделей по AIC

## 2(а) По значениям t-(или -z) статистики (по REML оценке)

Дает приблизительный результат; годится для факторов, если не больше 2 уровней

```{r}
summary(M1)
```

## 2(б) F-критерий - приблизительный результат (REML оценка)

Зависит от порядка включения предикторов в модель (Type I SS, sequential); годится если один предиктор, либо внимательно интерпретировать.

```{r}
anova(M1)
```

>- Вывод: Время реакции зависит от продолжительности бессонницы

## 2(в1) Попарное сравнение вложенных моделей при помощи likelihood ratio test 

Дает более точные выводы, чем F и t(z)

```{r}
M1.ml <- lme(Reaction ~ Days, random = ~1|Subject, data = sl, method = "ML")
M2.ml <- update(M1.ml, . ~ . - Days)
anova(M1.ml, M2.ml)
```

df теста - это разница df сравниваемых моделей = 4 - 3 = 1

> - Время реакции меняется в зависимости от продолжительности бессонницы (L = 116, df = 1, p < 0.01)

## 2(в2) Сравнение моделей по AIC

```{r}
AIC(M1.ml, M2.ml)
```

> - Т.к. AIC меньше у модели с `Days`, можем написать:

> - Продолжительность бессонницы влияет на время реакции (AIC)

## 3. Представление результатов

Для представления результатов переподбираем модель заново, используя Restricted Maximum Likelihood.

REML оценка параметров более точна

```{r}
MFinal1 <- lme(Reaction ~ Days, random = ~1|Subject, method = "REML", data = sl)
```

Для проверки финальной модели необходимо провести анализ остатков (те же графики, что и в п.1). Поскольку модель не изменилась, не привожу их здесь


## Теперь разберемся с допущениями модели {.smaller}

$$\mathbf{Reaction} _i = \mathbf{X} _i \times \boldsymbol{\beta} + \mathbf{Z} _i \times \mathbf{b} _i + \boldsymbol{\epsilon}_i$$

$\mathbf{b} _i \sim N(0, \mathbf{D})$ - случайные эффекты $b _i$ нормально распределены со средним 0 и матрицей ковариаций $\mathbf{D}$

$\boldsymbol{\epsilon} _i \sim N(0, \boldsymbol{\sigma} _i)$ - остатки модели нормально распределены со средним 0 и матрицей ковариаций $\boldsymbol{\sigma} _i$

Матрица ковариаций остатков выглядит так:
$$\mathbf{\sigma} _i = \sigma^2 \times 
\left( \begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{array} \right)$$

Т.е. остатки независимы друг от друга (вне диагонали стоят нули, т.е. ковариация разных остатков 0).

В то же время, отдельные значения переменной-отклика $Y _i$ уже не будут независимы друг от друга при добавлении случайных эффектов - см. ниже

## {.smaller}

Можно показать, что $Y _i$ нормально распределена $Y _i \sim N(\mathbf{X} _i \times \boldsymbol{\beta}, Cov(\mathbf{Y_i}) )$

$Cov(Y_i) = \mathbf{V} _i = \mathbf{Z} _i × \mathbf{D} × \mathbf{Z'} _i + \mathbf{\sigma} _i$ - матрица ковариаций зависимой переменной  
$\mathbf{D}$ - матрица ковариаций случайных эффектов.

Т.е. __добавление случайных эффектов приводит к изменению ковариационной матрицы__ $Cov(Y_i)$

Для модели со случайным intercept:

$$Cov(Y_i) = \left( \begin{array}{c} 1 \\ 1 \\ \vdots \\ 1 \end{array}\right)
\times d^2
\times \left( \begin{array}{c} 1 & 1 & \cdots & 1 \end{array}\right) +
\sigma^2
\times
\left( \begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{array} \right) =$$

$$
= \left( \begin{array}{cccc}
\sigma^2 + d^2 & d^2 & \cdots & d^2 \\
d^2 & \sigma^2 + d^2 & \cdots & d^2 \\
\vdots & \vdots & \ddots & \vdots \\
d^2 & d^2 & d^2 & \sigma^2 + d^2
\end{array} \right)
$$

## Индуцированная корреляция - следствие  включения в модель случайных эффектов
$$Cov(Y_i) =
\left( \begin{array}{cccc}
\sigma^2 + d^2 & d^2 & \cdots & d^2 \\
d^2 & \sigma^2 + d^2 & \cdots & d^2 \\
\vdots & \vdots & \ddots & \vdots \\
d^2 & d^2 & d^2 & \sigma^2 + d^2
\end{array} \right)
$$

$d^2$ - ковариация между наблюдениями одного субъекта; $\sigma^2 + d^2$ - дисперсия

Т.е. корреляция между наблюдениями одного субъекта $d^2 / (\sigma^2 + d^2)$

### Коэффициент внутриклассовой корреляции $d^2 / (\sigma^2 + d^2)$

Способ измерить, насколько коррелируют друг с другом наблюдения из одной и той же группы случайного фактора. Если он высок, то можно брать меньше проб в группе (и больше групп, если нужно)

## Вычисляем внутриклассовую корреляцию

$sigma_{Subject}^2 / (sigma_{Subject}^2 + sigma^2)$

```{r, eval=FALSE}
MFinal1
```

    В результатах
    Random effects:
     Formula: ~1 | Subject
            (Intercept) Residual
    StdDev:    37.12383 30.99123

```{r}
# Внутриклассовая корреляция
37.12383^2 / (37.12383^2 + 30.99123^2)
```

> - Значения времени реакции одного субъекта похожи, эффект субъекта нельзя игнорировать в анализе

## График предсказанных значений для результатов

```{r}
# 1) Новый датафрейм, для которого будем предсказывать
library(dplyr)
minmax <- sl %>% group_by(Subject) %>%
  summarise(mDays = min(Days), MDays = max(Days))
new_data <- minmax %>% group_by(Subject) %>%
  do(data.frame(Days = seq(.$mDays, .$MDays, length = 10)))

# 2) Матрица линейной модели
X <- model.matrix(~ Days, data = new_data)

# 3) Вычисляем предсказанные значения одним из двух способов
# level = 0 - для фиксированных эффектов (т.е. без учета субъекта)
new_data$.fitted <- predict(MFinal1, new_data, level = 0)
# или то же самое при помощи матриц
betas = fixef(MFinal1)
new_data$.fitted <- X %*% betas

# 4) Вычисляем стандартные ошибки предсказанных значений
# это квадратный корень из диагональных элементов
# матрицы ковариаций предсказанных значений X * cov(BETA) * t(X)
new_data$.se <- sqrt( diag(X %*% vcov(MFinal1) %*% t(X)) )
```

## 1-й вариант. График с предсказаниями по фиксированной части модели {.smaller}

```{r, tidy.opts=list(width = 60)}
ggplot(new_data) +
  geom_ribbon(alpha = 0.2, aes(x = Days, y = .fitted, ymin = .fitted - 1.98 * .se, ymax = .fitted + 1.98 * .se)) +
  geom_point(data = sl, aes(x = Days, y = Reaction))
```


## 2-й вариант. График с предсказаниями для индивидуальных уровней случайного фактора {.smaller}

```{r, tidy.opts=list(width = 60), fig.width=10}
# beta_0 + beta * Days + случайный эффект субъекта
new_data$.fitted1 <- predict(MFinal1, new_data, level = 1)
ggplot(new_data, aes(x = Days, y = .fitted1, group = Subject)) +
  geom_ribbon(aes(fill = Subject, ymin = .fitted1 - 1.98 * .se, ymax = .fitted1 + 1.98 * .se), alpha = 0.5) + geom_line() +
  geom_point(data = sl, aes(x = Days, y = Reaction))
  # попробуйте добавить facet_wrap(~Subject)
```

# смешаные модели со случайным интерсептом и углом наклона

## смешаная модель со случайным интерсептом и углом наклона

На графике индивидуальных эффектов было видно, что измерения для разных субъектов, возможно, идут непараллельными линиями. Усложним модель --- добавим случайные изменения угла наклона для каждого из субъектов

```{r}
MS1 <- lme(Reaction ~ Days, random = ~ 1 + Days|Subject, data = sl)
```

Дальнейшие действия по прежнему плану:
- Анализ остатков
- Проверка влияния факторов + подбор оптимальной модели
- Визуализация предсказаний

## Задание

Проверьте получившуюся модель MS1

Проведите самостоятельно:

- Анализ остатков
- Проверку влияния факторов + подбор оптимальной модели
- Визуализацию предсказаний

## 1) График остатков от предсказанных значений

```{r, tidy=FALSE}
# plot(M1)
sl$.stdresid1 <- resid(MS1, type = "pearson")
sl$.fitted1 <- fitted(MS1)
ggplot(sl) + geom_point(aes(x = .fitted1, y = .stdresid1, colour = Subject))
```

> - Есть большие остатки, гетерогенность дисперсий

## 2) График остатков от ковариат в модели {.smaller}

```{r, fig.width=10, tidy=FALSE}
p <- ggplot(data = sl, aes(y = .stdresid1))
grid.arrange(
  p + geom_point(aes(x = Days, colour = Subject)),
  p + geom_boxplot(aes(x = Subject)),
  ncol = 2)
```
> - Большой остаток у наблюдения 332 субъекта
> - Нелинейный паттерн
> - Гетерогенность дисперсий

## 2. Проверяем, какие из фиксированных факторов влияют при помощи likelihood ratio test 

```{r}
MS1.ml <- lme(Reaction ~ Days, random = ~1+Days|Subject, data = sl, method = "ML")
MS2.ml <- update(MS1.ml, random = ~1|Subject)
MS3.ml <- update(MS2.ml, .~.-Days)
anova(MS1.ml, MS2.ml, MS3.ml)
```

> - Время реакции меняется в зависимости от продолжительности бессонницы (L = 116, df = 1, p < 0.01). Скорость изменений зависит от субъекта (L = 42, df = 2)

## 3. Представление результатов

Для представления результатов переподбираем модель заново, используя Restricted Maximum Likelihood.

REML оценка параметров более точна

```{r}
MSFinal <- lme(Reaction ~ Days, random = ~1 + Days|Subject, method = "REML", data = sl)
```


```{r}
# 1) Новый датафрейм, для которого будем предсказывать
library(dplyr)
minmax <- sl %>% group_by(Subject) %>%
  summarise(mDays = min(Days), MDays = max(Days))
new_data <- minmax %>% group_by(Subject) %>%
  do(data.frame(Days = seq(.$mDays, .$MDays, length = 10)))
# 2) Матрица линейной модели
X <- model.matrix(~ Days, data = new_data)
# 3) Предсказанные значения
betas = fixef(MSFinal)
new_data$.fitted <- X %*% betas
# 4) Стандартные ошибки предсказанных значений
new_data$.se <- sqrt( diag(X %*% vcov(MSFinal) %*% t(X)) )
```

## 1-й вариант. График с предсказаниями по фиксированной части модели {.smaller}

```{r, tidy.opts=list(width = 60)}
ggplot(new_data) +
  geom_ribbon(alpha = 0.2, aes(x = Days, y = .fitted, ymin = .fitted - 1.98 * .se, ymax = .fitted + 1.98 * .se)) +
  geom_point(data = sl, aes(x = Days, y = Reaction))
```


## 2-й вариант. График с предсказаниями для индивидуальных уровней случайного фактора {.smaller}

```{r, tidy.opts=list(width = 60), fig.width=10}
# beta_0 + beta * Days + случайный эффект субъекта
new_data$.fitted1 <- predict(MSFinal, new_data, level = 1)
ggplot(new_data, aes(x = Days, y = .fitted1, group = Subject)) +
  geom_ribbon(aes(fill = Subject, ymin = .fitted1 - 1.98 * .se, ymax = .fitted1 + 1.98 * .se), alpha = 0.5) + geom_line() +
  geom_point(data = sl, aes(x = Days, y = Reaction))
  # попробуйте добавить facet_wrap(~Subject)
```


## Take home messages

- Смешаные модели включают случайные и фиксированные факторы
- Градации фиксированных факторов заранее определены, а выводы можно экстраполировать только на такие уровни, которые были задействованы в анализе. Тестируется гипотеза о значении средних
- Градации случайных факторов - выборка из возможных уровней, а выводы можно экстраполировать на другие уровни. Тестируется гипотеза о дисперсии между группами по фактору.
- Коэффициент внутриклассовой корреляции оценивает, насколько коррелируют друг с другом наблюдения из одной и той же группы случайного фактора

## Дополнительные ресурсы

- Crawley, M.J. (2007). The R Book (Wiley).
- Zuur, A.F., Ieno, E.N., Walker, N., Saveliev, A.A., and Smith, G.M. (2009). Mixed Effects Models and Extensions in Ecology With R (Springer).

